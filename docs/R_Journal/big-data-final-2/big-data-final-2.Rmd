---
title: '"Vinho Verde" Wines Quality Modeling'

author: 
  - name          : "Viviane Adohouannon"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/view.php?id=21444"  
  - name          : "Kate Alexander"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/view.php?id=21524"    
  - name          : "Diana Azbel"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/view.php?id=20687"  
  - name          : "Igor Baranov"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/profile.php?id=21219"
abstract: >
  Wine classification is a difficult task since taste is the least understood of the human senses. In this research we propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the wine certification step. We are using a dataset related to red and white Vinho Verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests. The classes are not balanced, there are much more normal wines than excellent or poor ones. The methods used to solve the problem are Random Forests, Clustering, Neural Networks and Support Vector Machine. Resuts of those methods applicaion are compared and analyzed.
output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
figsintext        : no
---

## Introduction

Data mining techniques aim at extracting knowledge from raw data. Several DM algorithms have been developed, each one with its own advantages and disadvantages \citep{witten_data_2011}. Those approaches have been applied to a large variety of problems, either for classification or regression. An interesting problem that has captured the attention of several researches is the prediction of wine quality \citep{CorCer09}. Wine industry is investing in new technologies for wine making and selling processes. A key issue in this context is wine certification which prevents the illegal adulteration and assures the wine quality. Wine certification is often assessed by physicochemical and sensory tests. The development of an accurate, computationally efficient and understandable prediction model can be of great utility for the wine industry. On the one hand, a good wine quality prediction can be very useful in the certification phase, since currently the sensory analysis is performed by human tasters, being clearly a subjective approach. An automatic predictive system can be integrated into a decision support system, helping the speed and quality of the oenologist performance. If it is concluded that several input variables are highly relevant to predict the wine quality, since in the production process some variables can be controlled, this information can be used to improve the wine quality. In this research wine taste preferences are modelled by algorithms. 

## Background

Portugal is a top ten wine exporting country with
3.17% of the market share in 2005 \citep{faostat}. Exports of its Vinho Verde wine (from
the northwest region) have increased by yearly. To support
its growth, the wine industry is investing in new technologies for both wine
making and selling processes. Wine certification and quality assessment are
key elements within this context. Certification prevents the illegal adulteration
of wines (to safeguard human health) and assures quality for the wine market.
Quality evaluation is often part of the certification process and can be used
to improve wine making (by identifying the most influential factors) and to
stratify wines such as premium brands (useful for setting prices).
Wine certification is generally assessed by physicochemical and sensory tests
\citep{teranishi_flavor_1999}. Physicochemical laboratory tests routinely used to characterize wine include
determination of density, alcohol or pH values, while sensory tests rely
mainly on human experts. It should be stressed that taste is the least understood
of the human senses, thus wine classification is a difficult task.
Moreover, the relationships between the physicochemical and sensory analysis
are complex and still not fully understood \citep{legin_evaluation_2003}.

## Objective

The objective of this project is is to provide a reliable and feasible recommendation algorithm to predict wine quality based on physicochemical tests.  The target value is a numeric value of wine 'quality', hence the task could be solved by several regression methods, like Random Forest, Support Vector Machines and Neural Networks. In addition, it was decided to apply Clustering Analysis to investigate if we could predict the quality better or if we could get any new knowledge from the dataset.

\newpage

# Data understanding

The two datasets presented in \citep{WineDataset} are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult \citep{CorCer09}. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones). 

```
The dataset contains 6497 observations:

Input variables (based on physicochemical tests): 
  1 - fixed acidity        (FA)
  2 - volatile acidity     (VA)
  3 - citric acid          (CA)
  4 - residual sugar       (RS)
  5 - chlorides            (CH)
  6 - free sulfur dioxide  (FSD)
  7 - total sulfur dioxide (TSD)
  8 - density              (DEN)
  9 - pH                   (pH)
  10 - sulphates           (SUL)
  11 - alcohol             (ALC) 

Output variable (based on sensory data): 
  12 - quality, based on sensory data (score between 0 and 10) - (QLT)
  13 - wine type (0 - red whine, 1 - white wine)
```

Let's try to make sence of those attributes. There are many so called "impact compounds" in wine defining its taste and smell. Some of them are obvious, leke ALC, DEN and RS. Some of them are really bizarre \citep{noauthor_weird_2017}. The chemical components mentioned in the list have the following influence on the wine taste.

First is "Type" - red wine is different from white wine \citep{busch_learn_2011}. They look different and they certainly taste different as well. The culprit in both cases: the skins, and a little something they bring to the party called tannins. Tannin provides the backbone of red wine, which is why you might describe a red wine as “firm” or “leathery” or just plain “bitter.” White wine has tannin, but not enough to make it the star of the show. Instead, white wines are backboned by acidity. That’s why you might say a wine is “crisp” or “tart.” 

Then there is volatile acidity (VA) that intensifies the taste of the other acids and tannins \citep{corison_must_1979}. As the name suggests it is referencing volatility in wine, which causes it to go bad. Acetic acid builds up in wine when there’s too much exposure to oxygen during winemaking and is usually caused by acetobacter (the vinegar-making bacteria!). VA is considered a fault at higher levels (1.4 g/L in red and 1.2 g/L in white) and can smell sharp like nail polish remover. But at lower levels, it can add fruity-smelling raspberry, passion fruit, or cherry-like flavors

Sulphur dioxide (SO2) is used to inhibit or kill unwanted yeasts and bacteria, and to protect wine from oxidation. Important concentrations of SO2 can affect the smell of the wine. It is also most-often noted on the finish, with some wines displaying a strong flavor of Sulphur after you've tasted (or swallowed) on the back of the mouth. Red wine contains less Sulphur Dioxide than white and rose as the above regulations show. Generally speaking, the drier the wine, the lesser the amount of SO2 it contains. \citep{noauthor_sulfites_nodate}.

In wine tasting, the general term “acidity” defined by pH, FA and CA, refers to the fresh, tart and sour attributes of the wine which are evaluated in relation to how well the acidity balances out the sweetness and bitter components of the wine such as tannins.

Sulfates (SUL) aren't involved in wine production, but some beer makers use calcium sulfate—also known as brewers' gypsum—to correct mineral deficiencies in water during the brewing process. Sulfites are naturally occurring compounds found in all wines; they act as a preservative by inhibiting microbial growth. 

The amount of CH in wine is influenced by both the terroir and type of grape \citep{coli_chloride_2015}, and the importance of quantification lies in the fact that wine flavor is strongly impacted by this particular ion, which, in high concentration, gives the wine an undesirable salty taste.

\newpage

## Data Preparation
To perform the analysis, certain R libraries were used. The code below was used to load and initialize the librarie, then loads the data.  To pretty-print the tables in this report we used xtable \citep{R-xtable} library.

```{r message=FALSE, warning=FALSE}
set.seed(42)
library(ggplot2)
library(reshape2)
library(plyr)
library(readr)
library(fpc)
library(data.table)
library(ggplot2)
```

```{r message=FALSE, warning=FALSE}
wines_red_data <- 
  read.csv(
    "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv",
    sep=";", 
    header = TRUE, 
    col.names = c("FA","VA","CA","RS","CH","FSD","TSD","DEN","pH","SUL","ALC","QLT"))

wines_red_data$TYPE <- 0
```

```{r message=FALSE, warning=FALSE}
wines_white_data <- 
  read.csv(
    "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv",
    sep=";", 
    header = TRUE, 
    col.names = c("FA","VA","CA","RS","CH","FSD","TSD","DEN","pH","SUL","ALC","QLT"))

wines_white_data$TYPE <- 1
```

```{r}
wines_data <- rbind(wines_red_data, wines_white_data)
```

## Preview of the data
Quick view of the data attributes statistics presented in the Table \ref{table:sum_rw}. For each attribute in the dataset this table shows min, max, mean and normal distribution 1st and 3rd quartiles values. The first rows of the dataset are presented in Table \ref{table:dhead10}.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
library(xtable)
options(xtable.floating = TRUE)
options(xtable.timestamp = "")
options(xtable.comment = FALSE)
print (xtable(head(wines_red_data, n = 20), 
  caption = "\\tt Red Wines Quality Dataset - first rows", label = "table:dhead10"), scalebox=.9)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
print(xtable(summary(wines_data[,1:6])), include.rownames = FALSE, scalebox=.9)
print(xtable(summary(wines_data[,7:12]), 
  caption = "\\tt Wines Dataset Attributes Summary", label = "table:sum_rw"),include.rownames = FALSE, scalebox=.9)
```

\newpage 

## Check for missing values
The dataset has no missing values. Code below calculate number of rows with missing values and checks if there is at list one.

```{r}
any(is.na(wines_data))
```

## Distribution of target value in the dataset
As we mentioned before, the target value QLT of the wine quality is not equally distributed. The Figure \ref{fig:hist_qlt_rw} demonstrates the distribution. As we can see, dataset covers mostly medium-quality wines with QLT between 5 and 7 well, low and high  quality wines represented poorly.

```{r}
prop.table(table(wines_data$QLT))
```

```{r hist_qlt_rw, fig.pos = 'h', fig.height=4, fig.width=5, fig.align="center", fig.cap="Distribution of Wine Quality Attribute"}
ggplot(data = wines_data, mapping = aes(x = QLT)) + geom_bar()
```

## Splitting the wine into train and test sets

The wines dataset has been split in such a way that train and test sets would have the same distribution of the QLT attribute. We used 70:30 split ratio. 
```{r message=FALSE, warning=FALSE}
library(caret)
cluster1 <- wines_data[,1:12]
train1.rows<- createDataPartition(y= cluster1$QLT, p=0.7, list = FALSE)
train1.data<- cluster1[train1.rows,]
prop.table((table(train1.data$QLT)))
```

```{r}
test1.data<- cluster1[-train1.rows,]
prop.table((table(test1.data$QLT)))
```

# Modelling of the Wine Quality

## Random Forests Prediction

First we use advanced but computationally demanding Random Forest method \citep{noauthor_random_2018}. Decision trees are a popular method for various machine learning tasks. Tree learning "come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate". In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.

```{r message=FALSE, warning=FALSE}
library(randomForest)
fitRF1 <- randomForest(
  QLT ~ ., method="anova",
  data=train1.data, importance=TRUE, ntree=500)
```

Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way. To measure the importance of the j-th feature after training, the values of the j-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set. The importance score for thej-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences.
Features which produce large values for this score are ranked as more important than features which produce small values. The Figure \ref{fig:forimp} presents this analysi.

```{r forimp, fig.width=5.5, fig.height=6, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Importance of the dataset attributes for the prediction of the QLT attribute"}
varImpPlot(fitRF1, main="")
```

The accuracy of the RF prediction is calsulated below. Table \ref{table:confMat2} presents re results of the RF analysis in the form of a confusion matrix. The viasual presentation of the calsulations is presented in Figure \ref{fig:plot_rf_rw}.

```{r}
PredictionRF1 <- predict(fitRF1, test1.data)
cor(PredictionRF1,test1.data$QLT)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
xtable(table(round(PredictionRF1),test1.data$QLT), caption = "\\tt Random Forest Pledictor Confusion Matrix", label = "table:confMat2")
```


```{r plot_rf_rw, fig.width=5.5, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Random Forest Prediction"}
library(ggplot2)
df2 = data.frame(as.factor(test1.data$QLT), PredictionRF1)
colnames(df2) <- c("Test","Prediction")
ggplot(df2, aes(x = Test, y = Prediction)) +
        geom_boxplot(outlier.colour = "red") +
        geom_jitter(width = 0.25, pch=20, col=rgb(0.1, 0.2, 0.8, 0.3))
```



## Support Vector Machine Clasification

### Create SVM Model and show summary
```{r}
library("e1071")
svm_model <- svm(QLT ~ ., data=train1.data)
summary(svm_model)
```

## Run SVM Prediction

The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences. First of all, because output is a real number it becomes very difficult to predict the information at hand, which has infinite possibilities. In the case of regression, a margin of tolerance (epsilon) is set in approximation to the SVM which would have already requested from the problem. But besides this fact, there is also a more complicated reason, the algorithm is more complicated therefore to be taken in consideration. However, the main idea is always the same: to minimize error, individualizing the hyperplane which maximizes the margin, keeping in mind that part of the error is tolerated.

```{r}
predSVM <- predict(svm_model, test1.data)
cor(predSVM,test1.data$QLT)
```


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
xtable(table(round(predSVM),test1.data$QLT), caption = "\\tt SVM Pledictor Confusion Matrix", label = "table:confMat3")
```


```{r plot_rf1_rw, fig.width=5.5, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="SVM Prediction"}
library(ggplot2)
df2 = data.frame(as.factor(test1.data$QLT), predSVM)
colnames(df2) <- c("Test","Prediction")
ggplot(df2, aes(x = Test, y = Prediction)) +
        geom_boxplot(outlier.colour = "red") +
        geom_jitter(width = 0.25, pch=20, col=rgb(0.1, 0.2, 0.8, 0.3))
```



## Neural Networks Prediction

[source](https://datascienceplus.com/fitting-neural-network-in-r/)

### Preparing scaled data
```{r}
set.seed(4231)
data <- wines_data[,1:12]
index <- sample(1:nrow(data),round(0.75*nrow(data)))
#index <- createDataPartition(y= data$QLT, p=0.5, list = FALSE)
maxs <- apply(data, 2, max) 
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]
```


```{r message=FALSE, warning=FALSE}
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("QLT ~", paste(n[!n %in% "QLT"], collapse = " + ")))
f
#nn <- neuralnet(f,data=train_,hidden=c(6,3),linear.output=F)
```

The Figure \ref{fig:plot_nn} demostrates the NN model with the weights on each connection.

```{r plot_nn, echo=TRUE, fig.height=9, fig.width=6, message=FALSE, warning=FALSE, fig.cap="Graphical representation of the NN model with the weights on each connection"}
#plot(nn)
knitr::include_graphics("images/nn_lg.png")
```


### Predicting whine quality using neural networks

NN outputs a normalized prediction, so we need to scale it back in order to make a meaningful comparison (or just a simple prediction)

```{r eval=FALSE, include=FALSE}
pr.nn <- compute(nn,test_[,1:11])
pr.nn_ <- pr.nn$net.result*(max(data$QLT)-min(data$QLT))+min(data$QLT)
test.r <- (test_$QLT)*(max(data$QLT)-min(data$QLT))+min(data$QLT)
cor(test.r,pr.nn_)
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, results='asis'}
xtable(table(round(pr.nn_),as.factor(test.r)), caption = "\\tt NN Pledictor Confusion Matrix", label = "table:confMat4")
```

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrrr}
  \hline
 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ 
  \hline
4 &   0 &   2 &   5 &   0 &   0 &   0 &   0 \\ 
  5 &   8 &  28 & 336 & 189 &  10 &   0 &   0 \\ 
  6 &   1 &  13 & 179 & 456 & 137 &  19 &   0 \\ 
  7 &   0 &   0 &   6 &  97 & 115 &  21 &   2 \\ 
   \hline
\end{tabular}
\caption{\tt NN Pledictor Confusion Matrix} 
\label{table:confMat4}
\end{table}

```

## [1] 0.6043741164
```

```{r fig.align = "center", out.width = "1\\textwidth"}
knitr::include_graphics("images/nn_scatter.png")
```

```{r plot_rf2_rw, eval=FALSE, fig.align="center", fig.cap="SVM Predictor Scatter Plot", fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(ggplot2)
df2 = data.frame(as.factor(test.r), pr.nn_)
colnames(df2) <- c("Test","Prediction")
ggplot(df2, aes(x = Test, y = Prediction)) +
        geom_boxplot(outlier.colour = "red") +
        geom_jitter(width = 0.25, pch=20, col=rgb(0.1, 0.2, 0.8, 0.3))
```

## Can we Guess Wine Type by its Biochemical Content?

```{r fig.align = "center", out.width = "0.75\\textwidth"}
knitr::include_graphics("images/vinho-verde-wine-region-20.jpg")
```

## Preparation for cluster analysis

### Wines dataset normalizing

Normalizing wine dataset in preparation for clustering
```{r}
wines_data.std <- scale(wines_data[1:11])
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
print(xtable(summary(head(wines_data.std[,1:6]))), include.rownames = FALSE, scalebox=1)
print(xtable(summary(head(wines_data.std[,7:11]))),include.rownames = FALSE, scalebox=1)
```

### Determine optimal number of clusters
First we need to determine number of clusters. 
Looking at the percentage of variance explained as: a function of the number of clusters, we should choose a number of clusters in order to ensure that too much modeling of the data is not given. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much more information (explains a lot of variance); but at some point, the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point. This method is called the 'elbow criterion'.
The diagram presented in Figure \ref{fig:elbow} demonstrates the 'elbow' curve. From this diagram we decided to use five (5) clusters in our analysis.

```{r elbow,  fig.height=4, fig.width=5.5, message=FALSE, warning=FALSE, fig.cap="Elbow Criterion Diagram"}
wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

wssplot(wines_data.std, nc=8) 
```

### Clustering using K-means method

k-means clustering \citep{noauthor_k-means_2018} is a method of vector quantization, which is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells \citep{noauthor_nouvelles_1908}.

The problem is computationally difficult (NP-hard), k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes. The algorithm has a loose relationship to the k-nearest neighbor classifier. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.

Resulting cluster centers are presented in Table \ref{table:kcent1}.

```{r}
set.seed(420)
clusters_num = 3
k.means.fit <- kmeans(wines_data.std, clusters_num,iter.max = 1000)
```


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
#library(xtable)
#options(xtable.floating = TRUE)
#options(xtable.timestamp = "")
#options(xtable.comment = FALSE)

dh.rescale <- xtable(k.means.fit$centers,
  caption = "\\tt K-means Resulting Cluster Centers", label = "table:kcent1")

print(dh.rescale, scalebox=1)
```




```{r fig.height=9, fig.width=6, fig.cap="2D representation of the Cluster solution"}
library(cluster)
clusplot(wines_data.std, k.means.fit$cluster, main='',
         color=TRUE, shade=FALSE,
         labels=clusters_num, lines=0)
```





# Explain clusters

## Explain by wine quality

Let's try to explain clusters by the wine quality. Code below builds a matrix whe columns are cluster numbers and rows are wine types. As we can see, quality does not explain clusters, it's evenly distributed among them.
QLT does not explain clusters, see Table \ref{table:clust_qlt}.

```{r results='asis'}
xtable(table(wines_data[,12],k.means.fit$cluster),
  caption = "\\tt Explaining Clusters by QLT", label = "table:clust_qlt")
```

## Explain by wine type

We have the following types:

 * 0 is red wine
 * 1 is red wine

Let's try to explain clusters by the wine type. Code below builds a matrix whe columns are cluster numbers and rows are wine types. As we can see, cluster 3 contains red wines, cluster 1 and 2 contain white wine, see Table \ref{table:clust_type1}.

```{r results='asis'}
centers <- table(wines_data[,13],k.means.fit$cluster)
rownames(centers) <- c("Red Wine", "White Wine")
xtable(centers,
  caption = "\\tt Explaining Clusters by Wine Type", label = "table:clust_type1")
```

Let's try to figure out how white wine clusters 1 and 2 differ from each other. Code below calculates a difference vector of cluster 1 and 2 and the sort the attributes in the order of the most influence:

```{r}
Difference <- k.means.fit$centers[1,] - k.means.fit$centers[2,]
Difference <- Difference[order(abs(Difference), decreasing = T)]
```

Let's find what are the most significant factors that separate group 1 from group 2. Code below calculates the different results presented in Table \ref{table:dif1}.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
dh.rescale <- xtable(data.frame(Difference),
  caption = "\\tt Difference between Clusters 1 and 1", label = "table:dif1")
print(dh.rescale, scalebox=1)
```

The table below shows that clusters 1 and 2 are mostly  differ in sweetness (RS), viscosity (DEN) and alcohol content (ALC). This make cluster 1 sweet wite wine group and cluster 2 contains dry white wines.

## More wine groups

Trying second "elbow" at 6.

```{r  fig.height=10, fig.width=10}
set.seed(420)
clusters_num = 6
k.means.fit <- kmeans(wines_data.std, clusters_num,iter.max = 1000)
k.means.fit$centers
k.means.fit$size
```

## Cluster Analysis K-Means - more wine types

```{r results='asis'}
centers <- table(wines_data[,13],k.means.fit$cluster)
rownames(centers) <- c("Red Wine", "White Wine")
xtable(centers,
  caption = "\\tt Explaining Clusters by Wine Type", label = "table:clust_type1")
```

\newpage

# Conclusion
Through exploring the Wine Quality dataset we developed an algorithm to predict the wine quality using its chemical characteristics and extracted some interesting information about the wines presented. 

First we applied the Random Forest Regresson methos and achieved accuracy of predicting wine quality of 72%. The method has shown that the most important attriutes that influence the quality estimation of wines by human experts are alcohol (ALC), volatile acidity (VA) and free sulfur dioxide (FSD). The RF method showed low precision in the area of poor and high quality wines. 

Next we applied a Support Vector Machine Regression method (SVM) and achieved accuracy of 62, lower than RF, but the precision in the area of poor and high quality wines increased at least 20%.

Next we applied we applied a Neural Networks (NN) regression cofigured with 11:6:3:1 layers and achieved overall accuracy of 60%, lower than both RF and SVM. Interestingly, the precision of NN in the area of poor and high quality wines was almost 10% higher than SVM, making NN the best method in quality prediction of the most interesting and least presented sector of wines in the dataset.

Finally we applied Cluster Analysis (CA) to investigate if we could predict the quality better or if we could get any new knowledge from the dataset. We discovered that there is no correlation between wine quality and biochemical data from the CA point of view. On the other hand, there is a strong correlation between clusters and wine types. Even though the information about influence of 11 chemical characteristics on the taste of wine is very limited, we discovered clusters that contain such types of wine as white sweet, white dry, rose and old dry red. We can add that more subtypes of wine were discovered, but it is diffucult to give them specific names using only the dataset at hand.

The project was a success. Next steps would be collecting more information about relation of the base chemical component on the wine taste and finding datasets with additional wine attributes, related more to the human interpretation of wines, and connets those datasets with the one used in the project.

\bibliography{RJreferences}

\newpage

# Note from the Authors
This file was generated using [_The R Journal_ style article template](https://github.com/rstudio/rticles), additional information on how to prepare articles for submission is here - [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf). The article itself is an executable R Markdown file that could be [downloaded from Github](https://github.com/ivbsoftware/scda1010-lab2/tree/master/docs/R_Journal/scda1010-lab2) with all the necessary artifacts.
