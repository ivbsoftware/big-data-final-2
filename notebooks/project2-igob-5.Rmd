---
title: "News Popularity Prediction"
output:
  html_document:
    df_print: paged
---

# Loading the dataset

```{r message=FALSE, warning=FALSE}
set.seed(42)
library(ggplot2)
library(reshape2)
library(plyr)
library(readr)
library(fpc)
library(data.table)
library(ggplot2)
library(dataPreparation)
```

## Datasets description

[Dataset description](http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)

## Reading the dataset

```{r}
news_data_full <- read.csv("../data/OnlineNewsPopularity.csv", header = TRUE)[,3:61]
#summary(news_data_full)
```
# Data exploration

## Check for missing values
The dataset has no missing values. Code below calculate number of rows with missing values and checks if there is at list one.

```{r}
any(is.na(news_data_full))
```

## Filter useless variables

The first thing to do, in order to make computation fast, would be to filter useless variables:

 * Constant variables
 * Variables that are in double (for example col1 == col2)
 * Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)

```{r message=FALSE, warning=FALSE}
library(dataPreparation)
constant_cols <- whichAreConstant(news_data_full)
double_cols <- whichAreInDouble(news_data_full)
bijections_cols <- whichAreBijection(news_data_full)
```


## Distribution of target value in the dataset
As we mentioned before, the target value QLT of the wine quality is not equally distributed. The Figure \ref{fig:hist_qlt_rw} demonstrates the distribution. As we can see, dataset covers mostly medium-quality wines with QLT between 5 and 7 well, low and high  quality wines represented poorly.

```{r hist_qlt_rw, fig.height=3, fig.width=6, fig.align="center", fig.cap="Distribution of Wine Quality Attribute"}

ggplot(news_data_full, aes(x=shares))  + geom_density()
```


```{r hist_qlt1_rw, fig.height=3, fig.width=6, fig.align="center", fig.cap="Distribution of Wine Quality Attribute"}

ggplot(news_data_full, aes(x=shares))  + geom_density() + scale_x_continuous(trans = 'log10')
```

```{r hist_qlt3_rw, fig.align="center", fig.cap="Distribution", fig.height=3, fig.width=6, message=FALSE, warning=FALSE}

ggplot(news_data_full, aes(x=shares))  + geom_density() + xlim(c(0, 10000))
```


## Discretization of target value
```{r}
news_data_full$shares_bins = news_data_full$shares
news_data_full <- fastDiscretization(dataSet = news_data_full, verbose = FALSE,
  bins = list(shares_bins = 
              c(0, 500, 1000, 2000, 3000, 4000, 5000, 7500, 10000, 25000, 50000, 100000, 1000000, +Inf)))
print(table(news_data_full$shares_bins))
```

## Discretization of target value by bins with equal frequency
```{r}
news_data_full$shares_bins_eqf = news_data_full$shares
bins <- build_bins(dataSet = news_data_full, cols = "shares_bins_eqf", 
                   n_bins = 12, type = "equal_freq", verbose = FALSE)
print(bins)
```
```{r}
news_data_full <- fastDiscretization(dataSet = news_data_full, bins = bins, verbose = FALSE)
print(table(news_data_full$shares_bins_eqf))
```


## Scale
Most machine learning algorithm rather handle scaled data instead of unscaled data. To perform scaling (meaning setting mean to 0 and standard deviation to 1), function fastScale is available. Since it is highly recommended to apply same scaling on train and test, you should compute the scales first using the function build_scales

```{r}
news_data_full$shares_bins_norm = news_data_full$shares
scales <- build_scales(dataSet = news_data_full, cols = c("shares_bins_norm"), verbose = TRUE)
print (scales)

news_data_full <- fastScale(dataSet = news_data_full, scales = scales, verbose = TRUE)
summary(news_data_full$shares_bins_norm)
```
```{r hist_qlt2_rw, echo=FALSE, fig.align="center", fig.cap="Distribution", fig.height=3, fig.width=6, message=FALSE, warning=FALSE}

ggplot(news_data_full, aes(x=shares_bins_norm))  + geom_density() + xlim(c(-1, 1))
```






## Use smaller subset for research
```{r message=FALSE, warning=FALSE}
library(caret)
news_data.rows<- createDataPartition(y= news_data_full$shares_bins_norm, p=0.3, list = FALSE)
news_data<- news_data_full[news_data.rows,]
```

## Stepwise Regression
Stepwise Regression method starts with the full model and eliminates predictors one at a time, at each step considering whether the criterion will be improved. As we see from the algorithm output, the new  model is identical to the one found the previous section manually.

```{r message=FALSE, warning=FALSE}
library(MASS)
fit <- lm(shares_bins_norm ~ . -shares - shares_bins - shares_bins_eqf, data=news_data)
step <- stepAIC(fit, direction="both", trace = FALSE)
step$anova

```

```{r}
summary(step$anova)
```

## Evaluating the model

```{r}
news_data.fit1 <- lm (
shares_bins_norm ~ n_tokens_title + n_tokens_content + n_unique_tokens + 
    n_non_stop_unique_tokens + num_hrefs + num_self_hrefs + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_tech + kw_min_min + 
    kw_max_min + kw_avg_min + kw_avg_max + kw_min_avg + kw_max_avg + 
    kw_avg_avg + self_reference_avg_sharess + weekday_is_monday + 
    LDA_02 + global_subjectivity + global_rate_positive_words + 
    min_positive_polarity + max_negative_polarity + title_subjectivity + 
    title_sentiment_polarity + abs_title_subjectivity,
    data=news_data)
fit1.sum <- summary(news_data.fit1)
fit1.sum
```
# Analyzing outliers with Cook’s Distance
[Cook’s distance](https://datascienceplus.com/outlier-detection-and-treatment-with-r/) is a measure computed with respect to a given regression model and therefore is impacted only by the X variables included in the model. But, what does cook’s distance mean? It computes the influence exerted by each data point (row) on the predicted outcome.


```{r}
cooksd <- cooks.distance(news_data.fit1)
```

## Influence measures
In general use, those observations that have a cook’s distance greater than 4 times the mean may be classified as influential. This is not a hard boundary.


```{r cooksd, fig.width=5, fig.height=9, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Influential Obs by Cooks distance"}

cooksd_cut_dist <- 4*mean(cooksd, na.rm=T)
cooksd_cut_dist_50 <- 50*mean(cooksd, na.rm=T)
cooksd_cut_dist_200 <- 200*mean(cooksd, na.rm=T)
plot(cooksd, pch="*", cex=1, main="", col = "green")  # plot cook's distance
abline(h = cooksd_cut_dist, col="red")  # add cutoff line
abline(h = cooksd_cut_dist_50, col="blue")  # add cutoff line
abline(h = cooksd_cut_dist_200, col="violet")  # add cutoff line
#text(x=1:length(cooksd)+1, y=cooksd, 
#     labels=ifelse(cooksd>cooksd_cut*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
```

## Re-evaluating the LR model without outliers

```{r message=FALSE, warning=FALSE}
library(MASS)
fit <- lm(shares ~ . 
          -shares_bins_norm - shares_bins - shares_bins_eqf,
          data=news_data[cooksd < cooksd_cut_dist])
step <- stepAIC(fit, direction="both", trace = FALSE)
step$anova

```


```{r}
news_data_cleaned4.fit <- lm (
shares ~ num_hrefs + num_self_hrefs + average_token_length + 
    data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed + 
    kw_max_min + kw_avg_max + kw_min_avg + kw_max_avg + kw_avg_avg + 
    self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday + 
    weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + 
    LDA_02 + rate_positive_words + min_negative_polarity + data_channel_is_tech,
    data=news_data[cooksd < cooksd_cut_dist])
fit2.sum <- summary(news_data_cleaned4.fit)
fit2.sum
```


## How Cooks distance distribute by bins

From the below we can see that most of the ouliers are in the case of 10K shares and more and they make less than 2% of all observations

```{r}
table(cooksd > cooksd_cut_dist)
table(news_data[,60][cooksd > cooksd_cut_dist])
```

## Analyzing outliers
If we extract and examine each influential row 1-by-1 (from below output), we will be able to reason out why that row turned out influential.


num_imgs 2
num_hrefs 1.6
data_channel_is_lifestyle: 2 
data_channel_is_world 1/3
data_channel_is_socmed 1/2
LDA_03 1.5
kw_max_min 3

weekday_is_saturday 1.5

```{r}

for(i in seq(1,58,3)) {
  print(summary(news_data[cooksd >= cooksd_cut_dist])[4,i:(i+2)])
  print(summary(news_data[cooksd < cooksd_cut_dist])[4,i:(i+2)])
  print("------------")
}
```


# Using RF Predictions

# Split data in test and train sets
```{r message=FALSE, warning=FALSE}
library(caret)
train1.rows<- createDataPartition(y= news_data$shares_bins, p=0.7, list = FALSE)
train1.data<- news_data[train1.rows,]
```
```{r message=FALSE, warning=FALSE}
attach(train1.data)
```

```{r}
test1.data<- news_data[-train1.rows,]
```

## Using RF Regressor

```{r message=FALSE, warning=FALSE}
library(randomForest)
fitRF1 <- randomForest(
  shares ~ . -shares_bins - shares_bins_eqf - shares_bins_norm,
  method="anova",
  data=train1.data, importance=TRUE, ntree=500)

#  -self_reference_avg_sharess -self_reference_max_shares- self_reference_min_shares, 

```

### Display importance table
```{r forimp, fig.width=8, fig.height=9, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Importance of the dataset attributes for the prediction of the 'class' attribute"}
varImpPlot(fitRF1, main="")
```

### Evaluating 
```{r}
PredictionRF1 <- predict(fitRF1, test1.data)
cor(PredictionRF1,test1.data$shares)
#confMat <- table(PredictionRF1,test1.data$shares_bins)
#confMat
#accuracy <- sum(diag(confMat))/sum(confMat)
#cat(sprintf("\nAccuracy=%f", accuracy))

```

## Using RF Classificator

```{r message=FALSE, warning=FALSE}
library(randomForest)
fitRF2 <- randomForest(
  shares_bins ~ . -shares - shares_bins_eqf - shares_bins_norm,
#  -self_reference_avg_sharess -self_reference_max_shares- self_reference_min_shares, 
  method="class",
  data=train1.data, importance=TRUE, ntree=500)
```

### Display importance table
```{r forimp2, fig.width=8, fig.height=9, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Importance of the dataset attributes for the prediction of the 'class' attribute"}
varImpPlot(fitRF2, main="")
```

### Evaluating 
```{r}
PredictionRF2 <- predict(fitRF2, test1.data)
confMat <- table(PredictionRF2,test1.data$shares_bins)
confMat
accuracy <- sum(diag(confMat))/sum(confMat)
cat(sprintf("\nAccuracy=%f", accuracy))
```

# Clustering using K-means
Trying to cluster data in order splitting set into more homogeneous sets

## Dataset normalizing

Normalizing dataset in preparation to clustering
```{r}
news_data.std <- scale(news_data[,1:58])
#summary(news_data.std)
```

## Chose number of clusters

A fundamental question is how to determine the value of the parameter k. 
If we looks at the percentage of variance explained as a function of the number of clusters: 
One should choose a number of clusters so that adding another cluster doesn't give much better 
modeling of the data. More precisely, if one plots the percentage of variance explained by the 
clusters against the number of clusters, the first clusters will add much information 
(explain a lot of variance), but at some point the marginal gain will drop, 
giving an angle in the graph. The number of clusters is chosen at this point, hence the "elbow criteria".


```{r}
wssplot <- function(data, nc=20, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

wssplot(news_data.std, nc=6) 
```

## Dataset clustering K-means
```{r  fig.height=10, fig.width=10}
clusters_num = 3
k.means.fit <- kmeans(news_data.std, clusters_num)
# attributes(k.means.fit)
k.means.fit$centers
# plot(k.means.fit$centers[,c("RS","ALC")])
# k.means.fit$cluster
k.means.fit$size
#pairs(~ALC+RS+pH+DEN, data=k.means.fit$centers)
```

```{r fig.height=10, fig.width=10, fig.cap="2D representation of the Cluster solution"}
library(cluster)
#clusplot(news_data.std, k.means.fit$cluster, main="",
#         color=TRUE, shade=TRUE,
#         labels=clusters_num, lines=0)
```


## Check if remote cluster corresponds to one of the bins

```{r}
table(news_data[,60][k.means.fit$cluster == 1])
```

## Explain the difference between cluster 1 and others

```{r}

for(i in seq(1,58,3)) {
  print(summary(news_data[k.means.fit$cluster == 1])[4,i:(i+2)])
  print(summary(news_data[k.means.fit$cluster != 1])[4,i:(i+2)])
  print("------------")
}
```

# Support Vector Machine Clasification

## Create SVM Model and show summary
```{r}
library("e1071")
svm_model <- svm(
shares_bins ~ num_hrefs + num_self_hrefs + average_token_length + 
    data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed + 
    kw_max_min + kw_avg_max + kw_min_avg + kw_max_avg + kw_avg_avg + 
    self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday + 
    weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + 
    LDA_02 + rate_positive_words + min_negative_polarity + data_channel_is_tech,
    data=train1.data)
summary(svm_model)
```

## Run Prediction
```{r}
predSVM <- predict(svm_model, test1.data)
confMat <- table(predSVM,test1.data$shares_bins)
confMat
accuracy <- sum(diag(confMat))/sum(confMat)
cat(sprintf("\nAccuracy=%f", accuracy))
```

# Neural Networks Prediction

[source](https://datascienceplus.com/fitting-neural-network-in-r/)

## Preparing scaled data
```{r}
set.seed(4231)
data <- news_data[,1:59]
index <- sample(1:nrow(data),round(0.75*nrow(data)))
#index <- createDataPartition(y= data$shares, p=0.75, list = FALSE)
maxs <- apply(data, 2, max) 
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]
```


```{r}
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("shares ~", paste(n[!n %in% "shares"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(10,5,3),linear.output=F)

```

```{r plot_nn, fig.height=10, fig.width=25, message=FALSE, warning=FALSE, fig.cap="Graphical representation of the NN model with the weights on each connection:"}
#plot(nn)
```

## Predicting shares using the neural network

NN outputs a normalized prediction, so we need to scale it back in order to make a meaningful comparison (or just a simple prediction)

```{r}
pr.nn <- compute(nn,test_[,1:58])
pr.nn_ <- pr.nn$net.result*(max(data$shares)-min(data$shares))+min(data$shares)
test.r <- (test_$shares)*(max(data$shares)-min(data$shares))+min(data$shares)
#MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
cor(test.r,pr.nn_)
#cor(pr.nn$net.result,test_$shares)
```

```{r}
par(mfrow=c(1,2))

plot(test.r,pr.nn_, col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')

plot(test.r,pr.nn_, col='red',main='Real vs predicted NN',pch=18,cex=0.7, xlim = c(0,20000))
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
```
